{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ4yqT7jNxmX"
      },
      "source": [
        "# **Install and Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pmbY9PC_2Xy",
        "outputId": "59740c8d-253c-415a-8438-a165ebe29e41"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf\n",
        "!pip install docx2pdf\n",
        "!pip install mammoth\n",
        "\n",
        "# !pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unywaH2INwbH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu8EcDam_3Ep"
      },
      "source": [
        "# **1. Scrape Website**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlHo5ej5TaUU"
      },
      "outputs": [],
      "source": [
        "def fetch_and_parse(url):\n",
        "    response = requests.get(url)\n",
        "    return BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "def get_header_keys(header_row):\n",
        "    headers = []\n",
        "    if header_row:\n",
        "        for header in header_row.find_all('th'):\n",
        "            headers.append(header.get_text(strip=True))\n",
        "    return headers\n",
        "\n",
        "def scrape_table(table, base_url):\n",
        "    data = []\n",
        "    header_row = table.find('tr', class_='colheader')\n",
        "    headers = get_header_keys(header_row)\n",
        "    rows = table.find_all('tr')\n",
        "    data_row_start = rows.index(header_row) + 1 if header_row else 0\n",
        "\n",
        "    for row in rows[data_row_start:]:\n",
        "        row_data = {}\n",
        "        cells = row.find_all('td')\n",
        "        for index, cell in enumerate(cells):\n",
        "            cell_key = headers[index] if index < len(headers) else f'Cell_{index}'\n",
        "            links = cell.find_all('a', href=True)\n",
        "            if links:\n",
        "                link_data = []\n",
        "                for link in links:\n",
        "                    href = link['href']\n",
        "                    link_text = link.get_text(strip=True)\n",
        "                    if 'docid' in href:\n",
        "                        link_data.append({'doc_url': base_url + href, 'title': link_text})\n",
        "                    elif 'id' in href or 'bid' in href:\n",
        "                        nested_url = base_url + href\n",
        "                        nested_soup = fetch_and_parse(nested_url)\n",
        "                        nested_tables = nested_soup.find_all('table')\n",
        "                        nested_data = scrape_table(nested_tables[0], base_url)\n",
        "                        link_data.append({'url': nested_url, 'title': link_text, 'nested_data': nested_data})\n",
        "                row_data[cell_key] = link_data\n",
        "            else:\n",
        "                row_data[cell_key] = cell.get_text(strip=True)\n",
        "        if row_data:\n",
        "          data.append(row_data)\n",
        "    return data\n",
        "\n",
        "start_url = 'https://kungorelse.nykarleby.fi:8443/ktwebbin/dbisa.dll/ktwebscr/pk_kokl_tweb.htm'\n",
        "base_url = 'https://kungorelse.nykarleby.fi:8443'\n",
        "soup = fetch_and_parse(start_url)\n",
        "tables = soup.find_all('table')\n",
        "result = scrape_table(tables[0], base_url)\n",
        "\n",
        "with open('/content/result1.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(result, f, ensure_ascii=False,indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UfsocfoAL9Q"
      },
      "source": [
        "# **2. Convert to DataFrame for Easier Access and Filtering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4EsfXZjFw6-"
      },
      "outputs": [],
      "source": [
        "# Load the JSON file into a Python object to explore its structure\n",
        "with open(\"/content/drive/MyDrive/result1.json\", \"r\") as file:\n",
        "    json_data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53Nx_IlnGx66"
      },
      "outputs": [],
      "source": [
        "columns = ['doc_link', 'rubrik', 'section', 'meeting_date', 'meeting_time', 'meeting_reference', 'verksamhetsorgan', 'parent_link']\n",
        "df = pd.DataFrame(columns = columns)\n",
        "\n",
        "for meeting in json_data:\n",
        "  for doc in meeting['Datum'][0]['nested_data']:\n",
        "    if 'Rubrik' in doc.keys() and isinstance(doc['Rubrik'][0], dict) and 'doc_url' in doc['Rubrik'][0].keys():\n",
        "      parent_row = {\n",
        "          'doc_link': doc['Rubrik'][0]['doc_url'],\n",
        "          'rubrik': doc['Rubrik'][0]['title'],\n",
        "          'section': \"\" if not doc['ยง'] else f\"ยง {doc['ยง']}\",\n",
        "          'meeting_date': meeting['Datum'][0]['title'].split(' ')[0].strip(),\n",
        "          'meeting_time': meeting['Datum'][0]['title'].split(' ')[1].strip(),\n",
        "          'meeting_reference': meeting['Verksamhetsorgan'].split(\":\")[1].strip(),\n",
        "          'verksamhetsorgan': meeting['Verksamhetsorgan'].split(\":\")[0].strip(),\n",
        "          'parent_link': \"\"\n",
        "      }\n",
        "      df = pd.concat([df, pd.DataFrame([parent_row])], ignore_index=True)\n",
        "      if 'Bilagor' in doc.keys() and doc['Bilagor'] and doc['Bilagor'] != '-':\n",
        "        for attachment in doc['Bilagor'][0]['nested_data']:\n",
        "          attachment_row = {\n",
        "              'doc_link':attachment['Cell_0'][0]['doc_url'],\n",
        "              'rubrik':attachment['Cell_0'][0]['title'],\n",
        "              'section':\"\",\n",
        "              'meeting_date': meeting['Datum'][0]['title'].split(' ')[0].strip(),\n",
        "              'meeting_time': meeting['Datum'][0]['title'].split(' ')[1].strip(),\n",
        "              'meeting_reference': meeting['Verksamhetsorgan'].split(\":\")[1].strip(),\n",
        "              'verksamhetsorgan': meeting['Verksamhetsorgan'].split(\":\")[0].strip(),\n",
        "              'parent_link': parent_row['doc_link']\n",
        "          }\n",
        "          df = pd.concat([df, pd.DataFrame([attachment_row])], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6G87msiASLI"
      },
      "source": [
        "# **3. Download all PDFs from links**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhezoz7hKvK0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "from urllib.parse import unquote\n",
        "import re\n",
        "\n",
        "# Create the 'protocols' directory if it doesn't exist\n",
        "download_path='/content/drive/MyDrive/all_protocols/protocols'\n",
        "os.makedirs(download_path, exist_ok=True)\n",
        "\n",
        "# Function to download PDF and return the filename\n",
        "def download_pdf(url):\n",
        "    try:\n",
        "        response = requests.get(url, allow_redirects=True)\n",
        "        response.raise_for_status()  # Raise an error for bad status codes\n",
        "\n",
        "        # Extract filename from Content-Disposition header\n",
        "        cd = response.headers.get('content-disposition','')\n",
        "        filename = re.findall('filename=(.+)', cd)\n",
        "        print(filename)\n",
        "        if filename:\n",
        "            filename = unquote(filename[0])  # Decoding any URL encoded characters\n",
        "        else:\n",
        "            # Create a filename if not found in the header\n",
        "            filename = url.split('/')[-1]\n",
        "\n",
        "        # Save the PDF in 'protocols' folder\n",
        "        filepath = os.path.join(download_path, filename)\n",
        "        with open(filepath, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        return filename\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error downloading {url}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Add the 'doc_name' column to the dataframe\n",
        "df['doc_name'] = df['doc_link'].apply(download_pdf)\n",
        "\n",
        "# sort columns\n",
        "df = df[['doc_name', 'doc_link', 'rubrik', 'section', 'meeting_date', 'meeting_time','meeting_reference',\n",
        "       'verksamhetsorgan', 'parent_link']]\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df.to_csv('/content/drive/MyDrive/all_protocols/metadata.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "eMsB0YWOMna1",
        "outputId": "13c3ef62-e685-455b-f780-d9d1a9f76ed4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/all_protocols/metadata.csv', index_col=0)\n",
        "df.fillna(\"\", inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOGZDiTjZ-GY"
      },
      "source": [
        "# **4. Extract XHTML from PDFs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6bmCFGjZ9kK",
        "outputId": "a272e77a-6349-4024-f638-404f0a014c11"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import os\n",
        "from mammoth import convert_to_html\n",
        "\n",
        "os.makedirs('/content/drive/MyDrive/all_protocols/protocols_html', exist_ok=True)\n",
        "for index, row in df[10:12].iterrows():\n",
        "  filename = row['doc_name']\n",
        "  file_rootname, file_extension = os.path.splitext(filename)\n",
        "  if file_extension == \".pdf\":\n",
        "    with fitz.open(f'/content/drive/MyDrive/all_protocols/protocols/{filename}') as doc:\n",
        "        text = \"\".join(page.get_text(\"xhtml\", flags=~fitz.TEXT_PRESERVE_IMAGES & fitz.TEXT_DEHYPHENATE & fitz.TEXT_PRESERVE_WHITESPACE) for page in doc)\n",
        "  elif file_extension == \".docx\":\n",
        "    with open(f'/content/drive/MyDrive/all_protocols/protocols/{filename}', 'rb') as docx:\n",
        "      text = convert_to_html(docx)\n",
        "      text = text.value\n",
        "\n",
        "  text = text.encode('utf-8')\n",
        "  with open(f'/content/drive/MyDrive/all_protocols/protocols_html/{file_rootname}.html', \"wb\") as file:\n",
        "    file.write(text)\n",
        "    print(text.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5oRI3w_NQ4v"
      },
      "source": [
        "# **4. Extract Meeting Metadata from PDF with LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2W9SWakWzP2"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import json\n",
        "import time\n",
        "from openai import OpenAI\n",
        "import multiprocessing\n",
        "\n",
        "# Initialize the OpenAI client and assistant_id\n",
        "client = OpenAI(api_key='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')\n",
        "assistant_id = \"asst_q3KCyxW9Ib7U1tinoHW7ynHy\"\n",
        "\n",
        "# Function to process text with LLM\n",
        "def extract_data_with_llm(text, pdf_name, assistant_id):\n",
        "    # Create a thread, send the extracted text, and run the assistant\n",
        "    thread = client.beta.threads.create()\n",
        "    message = client.beta.threads.messages.create(\n",
        "        thread_id=thread.id,\n",
        "        role=\"user\",\n",
        "        content=text\n",
        "    )\n",
        "    run = client.beta.threads.runs.create(\n",
        "        thread_id=thread.id,\n",
        "        assistant_id=assistant_id\n",
        "    )\n",
        "    while run.status != 'completed':\n",
        "        run = client.beta.threads.runs.retrieve(\n",
        "            thread_id=thread.id,\n",
        "            run_id=run.id\n",
        "        )\n",
        "        print(f'Status for {pdf_name}:', run.status)\n",
        "        time.sleep(5)\n",
        "\n",
        "    # Retrieve messages from the thread\n",
        "    messages = client.beta.threads.messages.list(\n",
        "        thread_id=thread.id\n",
        "    )\n",
        "    return messages.data[0].content[0].text.value.replace('```json', '').replace('```', '')\n",
        "\n",
        "# Function to process a single PDF\n",
        "def process_pdf(pdf_name):\n",
        "    print(f\"Processing: {pdf_name}\\n\")\n",
        "\n",
        "    # Open and read the PDF file\n",
        "    with fitz.open(f'/content/drive/MyDrive/all_protocols/protocols/{pdf_name}') as doc:\n",
        "        text = \"\".join(page.get_text(\"text\", flags=~fitz.TEXT_PRESERVE_IMAGES) for page in doc)\n",
        "\n",
        "    json_response = extract_data_with_llm(text, pdf_name, assistant_id)\n",
        "\n",
        "    # Check if the response is valid JSON; retry if not\n",
        "    for _ in range(3):  # Retry up to 3 times\n",
        "        try:\n",
        "            response_data = json.loads(json_response)\n",
        "            return pdf_name, response_data\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Invalid JSON response. Retrying...\")\n",
        "            json_response = extract_data_with_llm(text, pdf_name, assistant_id)\n",
        "\n",
        "    print(f\"Failed to get valid JSON response for {pdf_name} after retries.\")\n",
        "    return pdf_name, \"LLM Error!\"\n",
        "\n",
        "# Filter documents that contain meeting metadata\n",
        "filtered_df = df[df['rubrik'].isin(['Beslutande', 'Sammantrรคdesuppgifter och deltagande', 'Kokoustiedot ja osallistujat', 'Vln:Beslutande', 'Pรครคttรคjรคt'])]\n",
        "\n",
        "# Number of processes should be equal to the number of CPU cores\n",
        "num_processes = 2\n",
        "\n",
        "# Process each PDF in parallel\n",
        "with multiprocessing.Pool(num_processes) as pool:\n",
        "    results = pool.map(process_pdf, filtered_df['doc_name'])\n",
        "\n",
        "# Update the DataFrame with the results\n",
        "for pdf_name, metadata in results:\n",
        "    if metadata is not None:\n",
        "        df.loc[df['doc_name'] == pdf_name, 'metadata'] = metadata\n",
        "\n",
        "# Save or print the updated DataFrame\n",
        "print(df)\n",
        "# df.to_csv('updated_dataframe.csv')  # Optionally save to a CSV file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0FmCHUGvRsL",
        "outputId": "1eaafcb2-422b-4c9c-aa44-de84aabb4bce"
      },
      "outputs": [],
      "source": [
        "# inspect result format\n",
        "results[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCUAOqivt3KN"
      },
      "outputs": [],
      "source": [
        "# add metadata to dataframe\n",
        "for pdf_name, metadata in results:\n",
        "    if metadata:\n",
        "        df.loc[df['doc_name'] == pdf_name, 'meeting_end_time'] = metadata['endTime']\n",
        "        df.loc[df['doc_name'] == pdf_name, 'meeting_place'] = metadata['meetingPlace']\n",
        "        df.loc[df['doc_name'] == pdf_name, 'members'] = json.dumps(metadata['members'])\n",
        "        df.loc[df['doc_name'] == pdf_name, 'substitutes'] = json.dumps(metadata['substitutes'] if 'substitutes' in metadata.keys() else [])\n",
        "        df.loc[df['doc_name'] == pdf_name, 'additional_attendees'] = json.dumps(metadata['additionalAttendees'])\n",
        "        df.loc[df['doc_name'] == pdf_name, 'protocol_signatories'] = json.dumps(metadata['protocolSignatories'])\n",
        "        df.loc[df['doc_name'] == pdf_name, 'protocol_adjusters'] = json.dumps(metadata['protocolAdjustment']['adjustedBy'])\n",
        "        df.loc[df['doc_name'] == pdf_name, 'protocol_adjustment_date'] = metadata['protocolAdjustment']['adjustmentDate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3AhWdfDt3oL"
      },
      "outputs": [],
      "source": [
        "filtered_df = df[df['rubrik'].isin(['Beslutande', 'Sammantrรคdesuppgifter och deltagande', 'Kokoustiedot ja osallistujat', 'Vln:Beslutande', 'Pรครคttรคjรคt'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "fDEESUqGELwp",
        "outputId": "ab0f55a3-7111-4fac-e524-482f5caac9bf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/all_protocols/metadata.csv', index_col=0)\n",
        "df.fillna(\"\", inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emUYRFw6UCz0"
      },
      "outputs": [],
      "source": [
        "df.to_csv('/content/drive/MyDrive/all_protocols/metadata.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35G0VQUR_Mcw"
      },
      "source": [
        "# **5. Extract Proposals and Decisions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVjUrjkSK8IS"
      },
      "outputs": [],
      "source": [
        "non_agenda_titles = [\n",
        "    'Sammantrรคdets laglighet och beslutsfรถrhet',\n",
        "    'Godkรคnnande av fรถredragningslistan',\n",
        "    'Val av protokolljusterare',\n",
        "    'Sammantrรคdets laglighet och beslutfรถrhet',\n",
        "    'Sammantrรคdets konstituerande',\n",
        "    'Kokouksen laillisuus ja pรครคtรถsvaltaisuus',\n",
        "    'Kahden pรถytรคkirjantarkastajan valinta',\n",
        "    'Esityslistan hyvรคksyminen',\n",
        "    'Val av protokolljusterare och protokollfรถrare'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn0y0M3NAenX"
      },
      "outputs": [],
      "source": [
        "filtered_df = df[~df['rubrik'].isin(['Beslutande', 'Sammantrรคdesuppgifter och deltagande', 'Kokoustiedot ja osallistujat', 'Vln:Beslutande', 'Pรครคttรคjรคt', *non_agenda_titles])]\n",
        "filtered_df = filtered_df[(filtered_df['parent_link'] == \"\") & (~filtered_df['section'].isin([\"\", \"ยง 0\"]))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "xRDKLeEqpfPg",
        "outputId": "db59b8f6-3826-4c82-97b4-fa4cdd7984c4"
      },
      "outputs": [],
      "source": [
        "filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "qz6W360USfR9",
        "outputId": "eb3968e4-73bd-4528-e048-44a9b9c3b421"
      },
      "outputs": [],
      "source": [
        "stadsfullmรคktige_df = filtered_df[filtered_df['verksamhetsorgan'] == 'Stadsfullmรคktige']\n",
        "stadsfullmรคktige_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvw2VSH_g-lL",
        "outputId": "d377a5a5-d74b-4a41-9cad-147068db301a"
      },
      "outputs": [],
      "source": [
        "df.verksamhetsorgan.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKvbUG96HpmU",
        "outputId": "dccf1453-6ba5-4baf-d700-a4c55d27e7d7"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import docx\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# get unique organs\n",
        "organs = df.verksamhetsorgan.unique()\n",
        "\n",
        "# Construct the alternation group for organ names\n",
        "organ_alternation_group = '|'.join(re.escape(organ) for organ in organs)\n",
        "\n",
        "# Assemble the full regex pattern\n",
        "reference_regex = rf'({organ_alternation_group}) (\\d{{1,2}}\\.\\d{{1,2}}\\.\\d{{4}}),? (ยง \\d+|\\d+ ยง)'\n",
        "\n",
        "\n",
        "# Compile the regex pattern\n",
        "reference_compiled_regex = re.compile(reference_regex)\n",
        "\n",
        "print(reference_compiled_regex)\n",
        "\n",
        "\n",
        "for index, row in stadsfullmรคktige_df.iterrows():\n",
        "  date = row['meeting_date'].replace('.', r'\\.')\n",
        "  match_section = rf\"(?:(?:{row['verksamhetsorgan']} {date},? (?:{row['section']}|{row['section'].replace('ยง','').strip()} ยง)))(?:(?!(?:{row['verksamhetsorgan']} {date},? (?:{row['section']}|{row['section'].replace('ยง','').strip()} ยง))).)*$\"\n",
        "  filepath = f'/content/drive/MyDrive/all_protocols/protocols/{row[\"doc_name\"]}'\n",
        "  print(match_section)\n",
        "  print(row['doc_link'], row['verksamhetsorgan'], row['meeting_date'], row['section'])\n",
        "  try:\n",
        "    with fitz.open(filepath) as doc:\n",
        "      text = \"\".join(page.get_text(\"text\", flags=~fitz.TEXT_PRESERVE_IMAGES) for page in doc)\n",
        "  except:\n",
        "    doc = docx.Document(filepath)\n",
        "    text = '\\n'.join(para.text for para in doc.paragraphs)\n",
        "\n",
        "  matches = reference_compiled_regex.findall(text)\n",
        "  print(\"\\n\".join(set([match for match in [\" \".join(match) for match in matches]])))\n",
        "  print(\"-\"*100)\n",
        "\n",
        "  # Find the match\n",
        "  match = re.search(match_section, text, re.DOTALL)\n",
        "\n",
        "  if match:\n",
        "      print(\"Captured Text:\", match.group(0))\n",
        "  # print(text)\n",
        "  print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGjtv4vsDiHy"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "\n",
        "with fitz.open('/content/ungdomsfullmaktige_protokoll_09_05_2023_16_15_162526.pdf') as doc:\n",
        "      text = \"\".join(page.get_text(\"xhtml\", flags=~fitz.TEXT_PRESERVE_IMAGES) for page in doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "CqcACPoWD6ZX",
        "outputId": "bceca7e7-311f-42b3-ef1e-6ee472569849"
      },
      "outputs": [],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVT4uHqtEQil"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
