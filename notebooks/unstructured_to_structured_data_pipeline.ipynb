{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ4yqT7jNxmX"
      },
      "source": [
        "# **Install and Import Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ##### **Add the OpenAI API key in config/secrets.env file as follows:**\n",
        "\n",
        "> ###### **OPENAI_API_KEY = \"<api_key>\"**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unywaH2INwbH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "from neo4j import GraphDatabase\n",
        "# change dir to the root of the project\n",
        "\n",
        "# load config\n",
        "load_dotenv(\"../config/config.env\")\n",
        "\n",
        "# load secrets\n",
        "load_dotenv(\"../config/secrets.env\")\n",
        "\n",
        "from data_pipeline import *\n",
        "\n",
        "# change import dir to src\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "import llm_kg_retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu8EcDam_3Ep"
      },
      "source": [
        "# **1. Scrape Website**\n",
        "> Takes approximately 12 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlHo5ej5TaUU"
      },
      "outputs": [],
      "source": [
        "scrape_website()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6G87msiASLI"
      },
      "source": [
        "# **2. Download all PDFs from links**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhezoz7hKvK0"
      },
      "outputs": [],
      "source": [
        "download_documents()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOGZDiTjZ-GY"
      },
      "source": [
        "# **3. Extract HTML from PDFs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6bmCFGjZ9kK",
        "outputId": "a272e77a-6349-4024-f638-404f0a014c11"
      },
      "outputs": [],
      "source": [
        "# only convert pdf and docx files so it might be less than the downloaded files\n",
        "convert_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5oRI3w_NQ4v"
      },
      "source": [
        "# **4. Extract Meeting Metadata from PDF with LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type = \"agenda\"\n",
        "df = get_documents_dataframe(type=type)\n",
        "df = df[df[\"body\"] == \"Stadsfullm√§ktige\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# asynchronously extract meeting metadata, max 150 calls per minute (taking into account openai rate limits)\n",
        "await extract_meeting_data(df=df, type=type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35G0VQUR_Mcw"
      },
      "source": [
        "# **5. Extract Proposals and Decisions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Still not good output from LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "await extract_meeting_agenda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **6. Export JSON**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following code is used to construct individual metadata JSON files for each organs and meetings so it is easier to inspect and debug. It creates a folder called 'extracted_json' in the data directory and makes folder for each organ and meetings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_individual_json():\n",
        "    \"\"\"\n",
        "    Constructs the individual json files for each meeting item and meeting metadata\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(os.getenv(\"METADATA_FILE\"))\n",
        "    df.fillna(\"\", inplace=True)\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # create the path to the save the json files\n",
        "        save_path = os.path.join(os.getenv(\"EXTRACTED_JSON_PATH\"), row['body'], row['meeting_date'], row['doc_name'].split(\".\")[0])\n",
        "\n",
        "        if row['end_time'] != \"\":\n",
        "\n",
        "            # ensure that the path exists\n",
        "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "            # path to save the metadata json file\n",
        "            metadata_save_path = os.path.join(os.path.dirname(save_path), \"llm_meeting_metadata.json\")\n",
        "\n",
        "            json_data = f'''{{\n",
        "                    \"meeting_date\": \"{row['meeting_date']}\",\n",
        "                    \"start_time\": \"{row['start_time']}\",\n",
        "                    \"meeting_reference\": \"{row['meeting_reference']}\",\n",
        "                    \"end_time\": \"{row['end_time']}\",\n",
        "                    \"meeting_location\": \"{row['meeting_location']}\",\n",
        "                    \"participants\": {row['participants']},\n",
        "                    \"substitutes\": {row['substitutes']},\n",
        "                    \"additional_attendees\": {row['additional_attendees']},\n",
        "                    \"signed_by\": {row['signed_by']},\n",
        "                    \"adjusted_by\": {row['adjusted_by']},\n",
        "                    \"adjustment_date\": \"{row['adjustment_date']}\",\n",
        "                    \"meeting_items\": [] }}''' # meeting items is added when constructing the aggregate JSON file\n",
        "\n",
        "            json_data = json.dumps(json.loads(json_data), indent=4, ensure_ascii=False)\n",
        "            # save the metadata json file\n",
        "            with open(metadata_save_path, \"w\") as f:\n",
        "                f.write(json_data)\n",
        "                \n",
        "        elif row['agenda_metadata'] != \"\":\n",
        "            # ensure that the path exists\n",
        "            os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "            # path to save the meeting item json file\n",
        "            item_save_path = os.path.join(save_path, \"llm_meeting_item.json\")\n",
        "\n",
        "            item = json.loads(row['agenda_metadata'])\n",
        "            item['title'] = row['title']\n",
        "            item['section'] = row['section']\n",
        "\n",
        "            # get all the atachments of the row based on parent link\n",
        "            attachments = df[df['parent_link'] == row['doc_link']]\n",
        "\n",
        "            # add the attachments to the item\n",
        "            item['attachments'] = []\n",
        "            for index, attachment in attachments.iterrows():\n",
        "                item['attachments'].append({\n",
        "                    \"title\": attachment['title'],\n",
        "                    \"link\": attachment['doc_link']\n",
        "                })\n",
        "            # save the meeting item json file\n",
        "            with open(item_save_path, \"w\") as f:\n",
        "                f.write(json.dumps(item, indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "construct_individual_json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_aggregate_json(construct_from = \"llm\"): # construct_from = \"llm\" or \"manual\"\n",
        "    \"\"\"\n",
        "    Constructs a single JSON out of all the meeting metadata and items\n",
        "    \"\"\"\n",
        "    aggregate_json = {}\n",
        "    aggregate_json['body'] = []\n",
        "    extracted_json_path = os.getenv(\"EXTRACTED_JSON_PATH\")\n",
        "    if not os.path.exists(extracted_json_path):\n",
        "        print(\"Extracted JSON path does not exist\")\n",
        "        return\n",
        "    for body in os.scandir(extracted_json_path):\n",
        "        if not body.is_dir():\n",
        "            continue\n",
        "        aggregate_meeting = []\n",
        "        for meeting in os.scandir(body.path):\n",
        "            metadata_path = os.path.join(meeting.path, f\"{construct_from}_meeting_metadata.json\")\n",
        "            if os.path.exists(metadata_path):\n",
        "                with open(metadata_path, \"r\") as f:\n",
        "                    metadata = json.load(f)\n",
        "                for item in os.scandir(meeting.path):\n",
        "                    if item.is_dir():\n",
        "                        item_path = os.path.join(item.path, f\"{construct_from}_meeting_item.json\")\n",
        "                        if os.path.exists(item_path):\n",
        "                            with open(item_path, \"r\") as f:\n",
        "                                item = json.load(f)\n",
        "                            metadata['meeting_items'].append(item)\n",
        "                aggregate_meeting.append(metadata)\n",
        "        aggregate_json['body'].append({\n",
        "            \"name\": body.name,\n",
        "            \"meetings\": aggregate_meeting\n",
        "        })\n",
        "\n",
        "    with open(os.path.join(extracted_json_path, f\"{construct_from}_aggregate_data.json\"), \"w\") as f:\n",
        "        f.write(json.dumps(aggregate_json, indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "construct_aggregate_json() # construct_from = \"llm\" or \"manual\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sanity check for number of folders created. should correspond to number of meetings with metadata extracted with llm\n",
        "import glob\n",
        "len(glob.glob(os.getenv(\"EXTRACTED_JSON_PATH\") + \"/*/*/*\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **7. Create a Knowledge Graph from JSON**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup cohere api for embedding\n",
        "import cohere\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Generates embeddings for the input texts\n",
        "    \"\"\"\n",
        "    co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
        "    response = co.embed(texts=texts, model='embed-multilingual-v3.0', input_type=\"search_document\")  \n",
        "    return response.embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to execute Cypher queries\n",
        "def execute_cypher_queries(driver, data):\n",
        "    \"\"\"\n",
        "    Executes Cypher queries to create a knowledge graph in Neo4j\n",
        "\n",
        "    Args:\n",
        "        driver : neo4j driver\n",
        "        data : JSON data\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    with driver.session() as session:\n",
        "        # Delete existing nodes and relationships\n",
        "        print(\"Deleting existing nodes and relationships...\")\n",
        "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
        "\n",
        "        body_embeddings = generate_embeddings([body.get(\"name\", \"\") for body in data.get(\"body\", [])])\n",
        "        for i, body in enumerate(tqdm(data.get(\"body\", []), position=0, desc=\"Creating bodies\")):\n",
        "            # Merge Body\n",
        "            body_name = body.get(\"name\", \"\")\n",
        "            session.run(\"\"\"\n",
        "                MERGE (b:Body {name: $body_name})\n",
        "                SET b.name_embedding = $name_embedding\n",
        "                \"\"\", \n",
        "                body_name=body_name,\n",
        "                name_embedding=body_embeddings[i])\n",
        "\n",
        "            # Process meetings\n",
        "            meeting_embeddings = generate_embeddings([meeting.get(\"meeting_location\", \"\") for meeting in body.get(\"meetings\", [])])\n",
        "            for j, meeting in enumerate(tqdm(body.get(\"meetings\", []), position=1, leave=False, desc=\"Creating meetings\")):\n",
        "                # Merge Meeting\n",
        "                meeting_location = meeting.get(\"meeting_location\", \"\")\n",
        "                meeting_id = session.run(\"\"\"\n",
        "                    MERGE (m:Meeting {\n",
        "                    meeting_date: $meeting_date,\n",
        "                    start_time: $start_time,\n",
        "                    meeting_reference: $meeting_reference,\n",
        "                    end_time: $end_time,\n",
        "                    meeting_location: $meeting_location\n",
        "                    })\n",
        "                    WITH m\n",
        "                    MATCH (b:Body {name: $body_name})\n",
        "                    MERGE (b)-[:HOSTED]->(m)\n",
        "                    SET m.meeting_location_embedding = $meeting_location_embedding\n",
        "                    RETURN id(m)\n",
        "                    \"\"\", \n",
        "                    meeting_date=meeting.get(\"meeting_date\", \"\"),\n",
        "                    start_time=meeting.get(\"start_time\", \"\"),\n",
        "                    meeting_reference=meeting.get(\"meeting_reference\", \"\"),\n",
        "                    end_time=meeting.get(\"end_time\", \"\"),\n",
        "                    meeting_location=meeting_location,\n",
        "                    body_name=body_name,\n",
        "                    meeting_location_embedding=meeting_embeddings[j]\n",
        "                    ).single()[0]\n",
        "\n",
        "                # Process participants\n",
        "                for person in meeting.get(\"participants\", []):\n",
        "                    session.run(\"\"\"\n",
        "                        MERGE (p:Person {fname: $fname, lname: $lname})\n",
        "                        WITH p\n",
        "                        MATCH (m:Meeting) WHERE id(m) = $meeting_id\n",
        "                        MERGE (p)-[:ATTENDED {\n",
        "                        role: $role, \n",
        "                        attendance: coalesce($attendance, '')\n",
        "                        }]->(m)\n",
        "                        \"\"\", \n",
        "                        fname=person.get(\"fname\", \"\"),\n",
        "                        lname=person.get(\"lname\", \"\"),\n",
        "                        role=person.get(\"role\", \"\"),\n",
        "                        attendance=person.get(\"attendance\", \"\"),\n",
        "                        meeting_id=meeting_id)\n",
        "\n",
        "                # Process Substitutes\n",
        "                for substitute in meeting.get(\"substitutes\", []):\n",
        "                    session.run(\"\"\"\n",
        "                        // Create or find the substitute node and connect to the meeting\n",
        "                        MERGE (s:Person {fname: $fname, lname: $lname})\n",
        "                        WITH s\n",
        "                        MATCH (m:Meeting) WHERE id(m) = $meeting_id\n",
        "                        MERGE (s)-[:SUBSTITUTE_ATTENDEE]->(m)\n",
        "                        WITH s\n",
        "                        // Only proceed if substituted_for is not an empty string\n",
        "                        WHERE $substituted_for <> ''\n",
        "                        // Create or find the substituted person node and create a relationship\n",
        "                        MERGE (substituted:Person {name: $substituted_for})\n",
        "                        MERGE (s)-[:SUBSTITUTED_FOR]->(substituted)\n",
        "                        \"\"\", \n",
        "                        fname=substitute.get(\"fname\", \"\"),\n",
        "                        lname=substitute.get(\"lname\", \"\"),\n",
        "                        substituted_for=substitute.get(\"substituted_for\", \"\"),\n",
        "                        meeting_id=meeting_id)\n",
        "\n",
        "                # Process Additional Attendees\n",
        "                for attendee in meeting.get(\"additional_attendees\", []):\n",
        "                    session.run(\"\"\"\n",
        "                        MERGE (a:Person {fname: $fname, lname: $lname})\n",
        "                        WITH a\n",
        "                        MATCH (m:Meeting) WHERE id(m) = $meeting_id\n",
        "                        MERGE (a)-[:ADDITIONAL_ATTENDEE {\n",
        "                            role: coalesce($role, '')\n",
        "                        }]->(m)\n",
        "                    \"\"\", \n",
        "                        fname=attendee.get(\"fname\", \"\"),\n",
        "                        lname=attendee.get(\"lname\", \"\"),\n",
        "                        role=attendee.get(\"role\", \"\"),\n",
        "                        meeting_id=meeting_id)\n",
        "                    \n",
        "                                    # Process Signatories\n",
        "                for signatory in meeting.get(\"signed_by\", []):\n",
        "                    session.run(\"\"\"\n",
        "                        MERGE (s:Person {fname: coalesce($fname, ''), lname: coalesce($lname, '')})\n",
        "                        WITH s\n",
        "                        MATCH (m:Meeting) WHERE id(m) = $meeting_id\n",
        "                        MERGE (s)-[:SIGNED]->(m)\n",
        "                        \"\"\", \n",
        "                        fname=signatory.get(\"fname\", \"\"),\n",
        "                        lname=signatory.get(\"lname\", \"\"),\n",
        "                        meeting_id=meeting_id)\n",
        "                    \n",
        "                # Process Adjusters\n",
        "                for adjuster in meeting.get(\"adjusted_by\", []):\n",
        "                    name = adjuster.split(\" \")\n",
        "                    session.run(\"\"\"\n",
        "                        MERGE (a:Person {fname: coalesce($fname, ''), lname: coalesce($lname, '')})\n",
        "                        WITH a\n",
        "                        MATCH (m:Meetmng) WHERE id(m) = $meeting_id\n",
        "                        MERGE (a)-[:ADJUSTED]->(m)\n",
        "                        \"\"\", \n",
        "                        fname=\" \".join(name[:-1]) if len(name) > 2 else name[0],\n",
        "                        lname=name[-1],\n",
        "                        meeting_id=meeting_id)\n",
        "\n",
        "                # Process Meeting Items\n",
        "                for item in meeting.get(\"meeting_items\", []):\n",
        "                    item_embeddings = generate_embeddings([\n",
        "                        item.get(\"title\", \"\"),\n",
        "                        item.get(\"context\", \"\"),\n",
        "                        item.get(\"decision\", \"\")\n",
        "                    ])\n",
        "                    item_id = session.run(\"\"\"\n",
        "                        MERGE (i:MeetingItem {\n",
        "                            title: coalesce($title, ''),\n",
        "                            section: coalesce($section, ''),\n",
        "                            context: coalesce($context, ''),\n",
        "                            decision: coalesce($decision, '')\n",
        "                        })\n",
        "                        WITH i\n",
        "                        MATCH (m:Meeting) WHERE id(m) = $meeting_id\n",
        "                        MERGE (m)-[:HAS_ITEM]->(i)\n",
        "                        SET i.title_embedding = $title_embedding,\n",
        "                            i.context_embedding = $context_embedding,\n",
        "                            i.decision_embedding = $decision_embedding\n",
        "                        RETURN id(i)\n",
        "                        \"\"\", \n",
        "                        title=item.get(\"title\", \"\"),\n",
        "                        section=item.get(\"section\", \"\"),\n",
        "                        meeting_id=meeting_id,\n",
        "                        context=item.get(\"context\", \"\"),\n",
        "                        decision=item.get(\"decision\", \"\"),\n",
        "                        title_embedding=item_embeddings[0],\n",
        "                        context_embedding=item_embeddings[1],\n",
        "                        decision_embedding=item_embeddings[2]\n",
        "                        ).single()[0]\n",
        "\n",
        "                    # Process Preparers and Proposers similarly inside Meeting Items\n",
        "                    for preparer in item.get(\"prepared_by\", []):\n",
        "                        session.run(\"\"\"\n",
        "                            MERGE (p:Person {fname: coalesce($fname, ''), lname: coalesce($lname, '')})\n",
        "                            WITH p\n",
        "                            MATCH (i:MeetingItem) WHERE id(i) = $item_id\n",
        "                            MERGE (p)-[:PREPARED]->(i)\n",
        "                            \"\"\", \n",
        "                            fname=preparer.get(\"fname\", \"\"),\n",
        "                            lname=preparer.get(\"lname\", \"\"),\n",
        "                            item_id=item_id)\n",
        "                        \n",
        "                    for proposer in item.get(\"proposal_by\", []):\n",
        "                        session.run(\"\"\"\n",
        "                            MERGE (p:Person {fname: coalesce($fname, ''), lname: coalesce($lname, '')})\n",
        "                            WITH p\n",
        "                            MATCH (i:MeetingItem) WHERE id(i) = $item_id\n",
        "                            MERGE (p)-[:PROPOSED]->(i)\n",
        "                            \"\"\", \n",
        "                            fname=proposer.get(\"fname\", \"\"),\n",
        "                            lname=proposer.get(\"lname\", \"\"),\n",
        "                            item_id=item_id)\n",
        "                        \n",
        "                    # Process Meeting Item Attachments\n",
        "                    attachment_embeddings = generate_embeddings([attachment.get(\"title\", \"\") for attachment in item.get(\"attachments\", [])])\n",
        "                    for k, attachment in enumerate(item.get(\"attachments\", [])):\n",
        "                        session.run(\"\"\"\n",
        "                            MERGE (a:Attachment {link: coalesce($link, ''), title: coalesce($title, '')})\n",
        "                            WITH a\n",
        "                            MATCH (i:MeetingItem) WHERE id(i) = $item_id\n",
        "                            MERGE (i)-[:HAS_ATTACHMENT]->(a)\n",
        "                            SET a.title_embedding = $title_embedding\n",
        "                            \"\"\", \n",
        "                            link=attachment.get(\"link\", \"\"),\n",
        "                            title=attachment.get(\"title\", \"\"),\n",
        "                            title_embedding=attachment_embeddings[k],\n",
        "                            item_id=item_id)\n",
        "\n",
        "def create_embeddings_index(driver):\n",
        "    \"\"\"\n",
        "    Creates vector indexes for the embeddings in Neo4j\n",
        "\n",
        "    Args:\n",
        "        driver : neo4j driver\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    print(\"Creating vector indexes...\")\n",
        "    with driver.session() as session:\n",
        "            \n",
        "            # drop existing indexes\n",
        "            embedding_names = session.run(\"\"\"SHOW VECTOR INDEXES YIELD name\"\"\").to_df()[\"name\"].tolist()\n",
        "\n",
        "            # drop existing indexes\n",
        "            for name in embedding_names:\n",
        "                session.run(f\"DROP INDEX `{name}`\")\n",
        "\n",
        "            # define options for cypher query\n",
        "            options = \"\"\"OPTIONS {indexConfig: {\n",
        "                    `vector.dimensions`: 1024,\n",
        "                    `vector.similarity_function`: 'cosine'}}\"\"\"\n",
        "\n",
        "            # create vector index for each embedding\n",
        "            session.run(f\"\"\"\n",
        "                CREATE VECTOR INDEX `body_name_embedding` IF NOT EXISTS\n",
        "                FOR (b:Body) ON (b.name_embedding)\n",
        "                {options}\n",
        "            \"\"\")\n",
        "            session.run(f\"\"\"\n",
        "                CREATE VECTOR INDEX `meeting_location_embedding` IF NOT EXISTS\n",
        "                FOR (n:Meeting) ON (n.meeting_location_embedding)\n",
        "                {options}\n",
        "            \"\"\")\n",
        "\n",
        "            item_properties = [\"title_embedding\", \"context_embedding\", \"proposal_embedding\", \"decision_embedding\"]\n",
        "            for property in item_properties:\n",
        "                session.run(f\"\"\"\n",
        "                    CREATE VECTOR INDEX `item_{property}` IF NOT EXISTS\n",
        "                    FOR (n:MeetingItem) ON (n.{property})\n",
        "                    {options}\n",
        "                \"\"\")\n",
        "                \n",
        "            session.run(f\"\"\"\n",
        "                CREATE VECTOR INDEX `attachment_title_embedding` IF NOT EXISTS\n",
        "                FOR (n:Attachment) ON (n.title_embedding)\n",
        "                {options}\n",
        "            \"\"\")\n",
        "\n",
        "def post_process_knowledge_graph(driver):\n",
        "    \"\"\"\n",
        "    Post-processes the knowledge graph in Neo4j\n",
        "\n",
        "    Args:\n",
        "        driver : neo4j driver\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    print(\"Post-processing knowledge graph...\")\n",
        "    with driver.session() as session:\n",
        "        # convert date string (mm.dd.yyyy) to datetime\n",
        "        session.run(\"\"\"\n",
        "            MATCH (m:Meeting)\n",
        "            WHERE toString(m.meeting_date) = m.meeting_date\n",
        "            WITH m, \n",
        "                split(m.meeting_date, '.') AS dateParts\n",
        "            WITH m, \n",
        "                toInteger(dateParts[0]) AS day, \n",
        "                toInteger(dateParts[1]) AS month, \n",
        "                toInteger(dateParts[2]) AS year\n",
        "            SET m.meeting_date = datetime({ year: year, month: month, day: day })\n",
        "        \"\"\")\n",
        "\n",
        "        # TODO: Add logic to merge duplicate Person nodes with interchanged fname and lname, and also with variation in spelling of names  \n",
        "\n",
        "def get_index_info(driver):\n",
        "    with driver.session() as session:\n",
        "        res = session.run(\"\"\"\n",
        "                        SHOW VECTOR INDEXES YIELD name, labelsOrTypes, properties\n",
        "                            \"\"\")\n",
        "        return res.to_df().to_markdown()\n",
        "\n",
        "\n",
        "def create_knowledge_graph(constuct_from = \"llm\"): # construct_from = \"llm\" or \"manual\"\n",
        "    # Load JSON data\n",
        "    aggregate_json_path = os.path.join(os.getenv(\"EXTRACTED_JSON_PATH\"), f\"{constuct_from}_aggregate_data.json\")\n",
        "\n",
        "    with open(aggregate_json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Neo4j connection details\n",
        "    uri = os.getenv(\"NEO4J_URI\")\n",
        "    username = os.getenv(\"NEO4J_USERNAME\")\n",
        "    password = os.getenv(\"NEO4J_PASSWORD\")\n",
        "\n",
        "    # Connect to Neo4j\n",
        "    driver = GraphDatabase.driver(uri, auth=(username, password))\n",
        "\n",
        "    # # Execute Cypher queries\n",
        "    execute_cypher_queries(driver, data)\n",
        "\n",
        "    # Create embeddings index\n",
        "    create_embeddings_index(driver)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default it will construct the knowledge graph from LLM extracted data. If you want to construct it from manually created JSON data, then add the data manually as follows:\n",
        "\n",
        "1. Create a folder called 'extracted_json' in the data directory.\n",
        "2. Create inside the 'extracted_json' folder, create a JSON file and add the data. The data should follow the schema defined in data/schema/schema.json. You can find the example of dummy JSON data in data/schema/schema_example.json."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_knowledge_graph(constuct_from = \"llm\") # construct_from = \"llm\" or \"manual\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **8. Test data retrieval from Knowledge Graph with LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"How Many Meetings are there?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# instanciate the LLM query processor\n",
        "processor = llm_kg_retrieval.KnowledgeGraphRAG(\n",
        "                        url=os.getenv(\"NEO4J_URI\"),\n",
        "                        username=os.getenv(\"NEO4J_USERNAME\"),\n",
        "                        password=os.getenv(\"NEO4J_PASSWORD\"),\n",
        "                    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get response from LLM\n",
        "response, _, _= processor.process_prompt(prompt)\n",
        "print(\"Response:\", response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
