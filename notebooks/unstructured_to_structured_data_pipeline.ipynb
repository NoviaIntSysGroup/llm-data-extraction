{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ4yqT7jNxmX"
      },
      "source": [
        "# **Install and Import Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ##### **Add the OpenAI API key in config/secrets.env file as follows:**\n",
        "\n",
        "> ###### **OPENAI_API_KEY = \"<api_key>\"**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "unywaH2INwbH"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "# load config\n",
        "load_dotenv(\"../config/config.env\")\n",
        "\n",
        "# load secrets\n",
        "load_dotenv(\"../config/secrets.env\")\n",
        "\n",
        "from data_pipeline import *\n",
        "\n",
        "# change import dir to src\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "import llm_kg_retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu8EcDam_3Ep"
      },
      "source": [
        "# **1. Scrape Website**\n",
        "> Takes approximately 12 minutes to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> One can possibly use asyncronous functions to speed up this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlHo5ej5TaUU"
      },
      "outputs": [],
      "source": [
        "scrape_website()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6G87msiASLI"
      },
      "source": [
        "# **2. Download all meeting documents from the scraped links**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Takes 3+ hours to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> One can possibly use asyncronous functions to speed up this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhezoz7hKvK0"
      },
      "outputs": [],
      "source": [
        "download_documents()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOGZDiTjZ-GY"
      },
      "source": [
        "# **3. Extract HTML from PDFs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6bmCFGjZ9kK",
        "outputId": "a272e77a-6349-4024-f638-404f0a014c11"
      },
      "outputs": [],
      "source": [
        "# only convert pdf and docx files so it might be less than the downloaded files\n",
        "convert_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5oRI3w_NQ4v"
      },
      "source": [
        "# **4. Extract Meeting Metadata from documents with LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get dataframe for meeting metadata documents. One can filter the dataframe and extract metadata for specific documents only\n",
        "# the fetched dataframe consists of additional columns is_manual_metadata_extracted, is_llm_metadata_extracted \n",
        "# which shows if the data has already been extracted or not manually and with llm\n",
        "type = \"metadata\"\n",
        "metadata_df = get_documents_dataframe(type=type)\n",
        "metadata_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# asynchronously extract meeting metadata (taking into account openai rate limits; limit defined in config file)\n",
        "await extract_meeting_data(df=metadata_df, type=type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35G0VQUR_Mcw"
      },
      "source": [
        "# **5. Extract Agenda from documents with LLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Still not good output from LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get dataframe for meeting agenda documents. One can filter the dataframe and extract agenda for specific documents only\n",
        "# the fetched dataframe consists of additional columns is_manual_agenda_extracted, is_llm_agenda_extracted \n",
        "# which shows if the data has already been extracted or not manually and with llm\n",
        "type = \"agenda\"\n",
        "agenda_df = get_documents_dataframe(type=type)\n",
        "agenda_df = agenda_df[agenda_df[\"body\"] == \"Äldrerådet\"]\n",
        "agenda_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# asynchronously extract meeting metadata (taking into account openai rate limits; limit defined in config file)\n",
        "await extract_meeting_data(df=agenda_df, type=type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **6. Export JSON**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "construct_aggregate_json(construct_from=\"llm\", validate_json=True) # construct_from = \"llm\" or \"manual\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **7. Create a Knowledge Graph from JSON**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_knowledge_graph(construct_from = \"llm\") # construct_from = \"llm\" or \"manual\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default it will construct the knowledge graph from LLM extracted data. If you want to construct it from manually created JSON data, then add the data manually as follows:\n",
        "\n",
        "1. Manually create JSON files with extracted data inside respective folders in `data/protocols` folder and name it `manual_meeting_metadata.json` or `manual_meeting_agenda.json` depending on the document type. Folder structure is `<body>`/`<meeting_date>`/`<document>`. Put the JSON inside the `<document>` folder.\n",
        "\n",
        "2. Execute the `construct_aggregate_json(construct_from=\"manual\")` function. This will fail if the created JSON does not follow the schema defined in `data/schema/schema.json`\n",
        "\n",
        "3. Execute `create_knowledge_graph(constuct_from = \"manual\")` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **8. Test data retrieval from Knowledge Graph with LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Are there anything related to health?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# instanciate the LLM query processor\n",
        "processor = llm_kg_retrieval.KnowledgeGraphRAG(\n",
        "                        url=os.getenv(\"NEO4J_URI\"),\n",
        "                        username=os.getenv(\"NEO4J_USERNAME\"),\n",
        "                        password=os.getenv(\"NEO4J_PASSWORD\"),\n",
        "                    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get response from LLM\n",
        "response, _, _= processor.process_prompt(prompt)\n",
        "print(\"Response:\", response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
