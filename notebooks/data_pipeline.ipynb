{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ4yqT7jNxmX"
   },
   "source": [
    "# **Install and Import Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### **Make sure the secrets.env file is in the config folder. An example for secrets.env can be found in config/secrets_example.env file**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# load config\n",
    "load_dotenv(\"../config/config.env\")\n",
    "\n",
    "# load secrets\n",
    "load_dotenv(\"../config/secrets.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline import *\n",
    "import chatbot.llm_kg_retrieval as llm_kg_retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu8EcDam_3Ep"
   },
   "source": [
    "# **1. Scrape Website**\n",
    "> Takes approximately 12 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One can possibly use asyncronous functions to speed up this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlHo5ej5TaUU"
   },
   "outputs": [],
   "source": [
    "scrape_website()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6G87msiASLI"
   },
   "source": [
    "# **2. Download all meeting documents from the scraped links**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One can possibly use asyncronous functions to speed up this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhezoz7hKvK0"
   },
   "outputs": [],
   "source": [
    "download_documents(overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOGZDiTjZ-GY"
   },
   "source": [
    "# **3. Extract HTML and text from PDFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6bmCFGjZ9kK",
    "outputId": "a272e77a-6349-4024-f638-404f0a014c11"
   },
   "outputs": [],
   "source": [
    "# only converts pdf and docx files so it might be less than the downloaded files\n",
    "convert_files(output_type=\"xhtml\", overwrite=True, add_ids_to_tags=True)\n",
    "convert_files(output_type=\"text\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5oRI3w_NQ4v"
   },
   "source": [
    "# **4. Extract Meeting Metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframe for meeting metadata documents. One can filter the dataframe and extract metadata for specific documents only\n",
    "# the fetched dataframe consists of additional columns is_manual_metadata_extracted, is_llm_metadata_extracted \n",
    "# which shows if the data has already been extracted or not manually and with llm\n",
    "metadata_df = get_documents_dataframe(type=\"metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# asynchronously extract meeting metadata (taking into account openai rate limits; limit defined in config file)\n",
    "metadata_batch_id, _ = extract_meeting_data_batch(df=metadata_df, type=\"metadata\", overwrite_data=True)\n",
    "time.sleep(3)\n",
    "\n",
    "# Check batch status\n",
    "print(\"Status for batch metadata extraction:\")\n",
    "metadata_output_id = None\n",
    "while metadata_output_id is None:\n",
    "    metadata_output_id = check_batch_status(metadata_batch_id)\n",
    "    time.sleep(1)\n",
    "metadata_output_jsonl = retrieve_batch_output(metadata_output_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await save_metadata_llm_batch_results(metadata_output_jsonl, metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35G0VQUR_Mcw"
   },
   "source": [
    "# **5. Extract Meeting Agenda**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agenda_df = get_documents_dataframe(type=\"agenda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asynchronously extract meeting agenda (taking into account openai rate limits; limit defined in config file)\n",
    "# await extract_meeting_data(df=agenda_df, type=type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch extract meeting agenda for body\n",
    "import time\n",
    "agenda_batch_id, references_batch_id = extract_meeting_data_batch(df=agenda_df, type=\"agenda\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Check batch status\n",
    "print(\"Status for batch agenda extraction:\")\n",
    "agenda_output_id = None\n",
    "while agenda_output_id is None:\n",
    "    agenda_output_id = check_batch_status(agenda_batch_id)\n",
    "    time.sleep(1)\n",
    "agenda_output_jsonl = retrieve_batch_output(agenda_output_id)\n",
    "\n",
    "print(\"Status for batch references extraction:\")\n",
    "references_output_id = None\n",
    "while references_output_id is None:\n",
    "    references_output_id = check_batch_status(references_batch_id)\n",
    "    time.sleep(1)\n",
    "references_output_jsonl = retrieve_batch_output(references_output_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save batch results\n",
    "await save_agenda_llm_batch_results(agenda_output_jsonl, agenda_df, references_jsonl=references_output_jsonl)\n",
    "\n",
    "# Create html to preview the extracted agenda data, saved in notebooks folder\n",
    "create_agenda_html(agenda_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Export JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_aggregate_json(construct_from=\"llm\") # construct_from = \"llm\" or \"manual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Create a Knowledge Graph from JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caution: This will completely overwrite the current knowledge graph, if any\n",
    "create_knowledge_graph(construct_from = \"llm\") # construct_from = \"llm\" or \"manual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default it will construct the knowledge graph from LLM extracted data. If you want to construct it from manually created JSON data, then add the data manually as follows:\n",
    "\n",
    "1. Manually create JSON files with extracted data inside respective folders in `data/protocols` folder and name it `manual_meeting_metadata.json` or `manual_meeting_agenda.json` depending on the document type. Folder structure is `<body>`/`<meeting_date>`/`<document>`. Put the JSON inside the `<document>` folder.\n",
    "\n",
    "2. Execute the `construct_aggregate_json(construct_from=\"manual\")` function. This will fail if the created JSON does not follow the schema defined in `data/schema/schema.json`\n",
    "\n",
    "3. Execute `create_knowledge_graph(constuct_from = \"manual\")` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. Test data retrieval from Knowledge Graph with LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Are there any errands involving cats?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the LLM query processor\n",
    "processor = llm_kg_retrieval.KnowledgeGraphRAG(\n",
    "                url=os.getenv(\"NEO4J_URI\"),\n",
    "                username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "                password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get response from LLM\n",
    "response, _, _= processor.process_prompt(prompt)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
